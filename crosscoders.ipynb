{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosscoders\n",
    "Attempting to recreate the SAE model Anthropic wrote about recently.\n",
    "\n",
    "Its a SAE which is layer and model agnostic.\n",
    "\n",
    "I'm not sure about the model agnostic part, but I have an idea about the layer agnostic part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-0.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-0.5B\").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "tokens = tokenizer.encode(\"print(\\\"Hello\", return_tensors=\"pt\").to(DEVICE)\n",
    "generation = model.generate(tokens, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(generation[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can very simply get the hidden states by setting output_hidden_states=True flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(tokens, output_hidden_states=True)\n",
    "\n",
    "# train SAE on these\n",
    "len(out.hidden_states), out.hidden_states[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can register hooks to modify the activations as we please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.model.layers[0].mlp.up_proj\n",
    "\n",
    "hook = layer.register_forward_hook(lambda module, input, output: output * 0)\n",
    "\n",
    "generation = model.generate(tokens, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(generation[0], skip_special_tokens=True))\n",
    "\n",
    "hook.remove()\n",
    "\n",
    "generation = model.generate(tokens, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(generation[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = model.config.num_hidden_layers\n",
    "HIDDEN_SIZE = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these models are specifically trained on specific chat formats\n",
    "# the (system, user, assistant) format is just a fancy string in the end\n",
    "def format_to_chat(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": example['prompt']},\n",
    "        {\"role\": \"assistant\", \"content\": example['response']}\n",
    "    ]\n",
    "    \n",
    "    chat_format = tokenizer.apply_chat_template(messages, tokenize=True)\n",
    "    return {\"chat_ids\": chat_format}\n",
    "\n",
    "\n",
    "train_ds = load_dataset(\"nampdn-ai/tiny-codes\", split=\"train\").shuffle(seed=42)\n",
    "train_ds = train_ds.map(format_to_chat, num_proc=8)\n",
    "\n",
    "val_ds = train_ds.take(len(train_ds) // 20)\n",
    "train_ds = train_ds.skip(len(train_ds) // 20)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(train_ds[0][\"chat_ids\"])[None].to(DEVICE) # add batch dimension\n",
    "attn = torch.tril(torch.ones(1, x.shape[1], x.shape[1])).bool().to(DEVICE)\n",
    "\n",
    "out = model(x, attention_mask=attn, output_hidden_states=True)\n",
    "\n",
    "out.hidden_states[0].reshape(-1, HIDDEN_SIZE).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inform the SAE of which layer it is being applied to, we encode the layer position into the activations via a sinusoidal positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_len):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2) * (-math.log(10000.0) / hidden_size))\n",
    "        \n",
    "        pe = torch.zeros(max_len, hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        return x + self.pe[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse autoencoder itself is quite simple. It's a traditional autoencoder, but with a twist: instead of 'compressing' the input through a bottleneck, it's overcomplete - meaning it has a larger hidden size than the input size.\n",
    "\n",
    "There are different ways of achieving sparsity in these activations. One popular formulation uses a top_k activation function to ensure only the k most important features remain active, while others methods directly optimize for sparsity through various loss terms.\n",
    "\n",
    "The model is trained to reconstruct its input from these sparse activations using reconstruction loss. We can also evaluate its effectiveness by measuring \"recovered\" loss - essentially comparing the next-token prediction accuracy (via negative log-likelihood) both before and after applying the SAE intervention. This helps us understand how well the SAE preserves the important information from the original representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 32\n",
    "\n",
    "class TopKSAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, top_k):\n",
    "        super().__init__()\n",
    "        self.input_size, self.hidden_size, self.n_layers, self.top_k = input_size, hidden_size, n_layers, top_k\n",
    "        \n",
    "        self.pos_enc = SinusoidalPositionalEncoding(input_size, max_len=n_layers)\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def encode(self, x, pos):\n",
    "        x = self.pos_enc(x, pos)\n",
    "        x = self.encoder(x)\n",
    "        top_k = torch.topk(x, k=self.top_k)\n",
    "        return top_k.indices, top_k.values\n",
    "    \n",
    "    def decode(self, indices, values):\n",
    "        sparse = torch.zeros(indices.shape[0], self.hidden_size).to(indices.device)\n",
    "        sparse.scatter_(1, indices, values)\n",
    "        return self.decoder(sparse)\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        indices, values = self.encode(x, pos)\n",
    "        return self.decode(indices, values)\n",
    "    \n",
    "\n",
    "\n",
    "sae = TopKSAE(HIDDEN_SIZE, HIDDEN_SIZE * 2, N_LAYERS, K).to(DEVICE)\n",
    "activations = out.hidden_states[0].reshape(-1, HIDDEN_SIZE)\n",
    "positions = torch.tensor([0] * len(activations))\n",
    "sae(activations, positions).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am borrowing an idea from diffusion models and will encode at which layer in the transformer model, the SAE is being applied to (in diffusion it would be the level of noise being on the image that it needs to denoise).\n",
    "\n",
    "Here I implement a sinusoidal positional encoding for the layers. One could also use learnable embeddings, but I fear that would \"divide\" the expressiveness of the SAE between these embeddings and the SAE itself. Using static Sinusoidal PE sounds like a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train\n",
    "\n",
    "Firstly, I dislike the habit people have of doing an entire training epoch before validating (especially since datasets can be huge). So I more often than not do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16\n",
    "MAX_SEQ_LEN = 64\n",
    "\n",
    "\n",
    "def cycle(dl):\n",
    "    while True: yield from dl\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item[\"chat_ids\"][:MAX_SEQ_LEN]) for item in batch]\n",
    "    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "    B, L = input_ids.shape\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": torch.tril(torch.ones(B, 1, L, L)).float()\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "N_STEPS_PER_TRAIN_EPOCH = len(train_loader)\n",
    "N_STEPS_PER_VAL_EPOCH = len(val_loader)\n",
    "\n",
    "# per epoch\n",
    "INTERLEAVE = 100\n",
    "INTERLEAVE_EVERY = N_STEPS_PER_TRAIN_EPOCH // INTERLEAVE\n",
    "N_VALIDATION_STEPS = N_STEPS_PER_VAL_EPOCH // INTERLEAVE\n",
    "\n",
    "SAVE_EVERY = 1000\n",
    "\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, it feels almost wasteful to only use one random layer per item.\n",
    "\n",
    "But perhaps using multiple layers per position would actually hurt training.\n",
    "\n",
    "Adjacent layers process the same token through successive transformations (and has skip connections), which creates highly correlated features. This correlation is perhaps stronger than the relationship between different positions in the sequence. Using multiple layers per position would likely reduce our batch diversity, as we'd be training on variations of the same underlying features rather than truly independent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optim = Adam(sae.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "train_loss, val_loss = float(\"inf\"), float(\"inf\")\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "def sae_step(batch):\n",
    "    B, L = batch[\"input_ids\"].shape\n",
    "    with torch.no_grad():\n",
    "        out = model(batch[\"input_ids\"].to(DEVICE), attention_mask=batch[\"attention_mask\"].to(DEVICE), output_hidden_states=True)\n",
    "\n",
    "    # Generate random layer for each position in the batch/sequence\n",
    "    random_layers = torch.randint(0, N_LAYERS, (B * L,))\n",
    "    \n",
    "    # Gather hidden states from random layers for each position\n",
    "    all_hidden = torch.stack(out.hidden_states)  # [n_layers, batch_size, seq_len, hidden_size]\n",
    "    all_hidden = all_hidden.permute(1, 2, 0, 3)  # [batch_size, seq_len, n_layers, hidden_size]\n",
    "    \n",
    "    # Create indices for gathering\n",
    "    batch_indices = torch.arange(B).repeat_interleave(L)\n",
    "    seq_indices = torch.arange(L).repeat(B)\n",
    "    \n",
    "    # Gather the hidden states using the random layers\n",
    "    inputs = all_hidden[batch_indices, seq_indices, random_layers]  # [batch_size*seq_len, hidden_size]\n",
    "    \n",
    "    reconstructed = sae(inputs, random_layers)\n",
    "    \n",
    "    return F.mse_loss(reconstructed, inputs)\n",
    "\n",
    "step_iter = tqdm(range(EPOCHS * N_STEPS_PER_TRAIN_EPOCH), desc=\"Training\")\n",
    "for step in step_iter:\n",
    "    batch = next(train_iter)\n",
    "    \n",
    "    loss = sae_step(batch)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    step_iter.set_postfix({\"Train Loss\": f\"{train_loss:.4f}\", \"Val Loss\": f\"{val_loss:.4f}\"})\n",
    "    \n",
    "    if step % INTERLEAVE_EVERY == 0:\n",
    "        interleaved_losses = []\n",
    "        for _ in range(N_VALIDATION_STEPS):\n",
    "            batch = next(val_iter)\n",
    "            with torch.no_grad():\n",
    "                loss = sae_step(batch)\n",
    "                interleaved_losses.append(loss.item())\n",
    "                \n",
    "                val_loss = loss.item()\n",
    "                \n",
    "                step_iter.set_postfix({\"Train Loss\": f\"{train_loss:.4f}\", \"Val Loss\": f\"{val_loss:.4f}\"})\n",
    "\n",
    "        val_loss = sum(interleaved_losses) / len(interleaved_losses)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    if step % SAVE_EVERY == 0 and step > 0:\n",
    "        torch.save(sae.state_dict(), f\"sae_{step}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(train_losses)), train_losses, label='Train Loss', alpha=0.5)\n",
    "# Ensure validation points align with training points by using same length arrays\n",
    "val_steps = range(0, len(train_losses), INTERLEAVE_EVERY)\n",
    "val_losses_padded = val_losses + [val_losses[-1]] * (len(val_steps) - len(val_losses))\n",
    "plt.plot(val_steps, val_losses_padded, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still trending downwards, only trained this for 20-30 minutes on my mac. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to see whether the SAE is picking up on something interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = val_ds.shuffle(seed=42).select(range(100))\n",
    "subset = [torch.tensor(x[\"chat_ids\"])[None] for x in subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = N_LAYERS // 2  # maybe the middle is interesting\n",
    "\n",
    "with torch.no_grad():\n",
    "    activations = [model(x.to(DEVICE), output_hidden_states=True).hidden_states[LAYER].reshape(-1, HIDDEN_SIZE) for x in tqdm(subset)]\n",
    "\n",
    "    activations = torch.cat(activations, dim=0)\n",
    "    positions = torch.tensor([LAYER] * len(activations))\n",
    "    \n",
    "    indices, values = sae.encode(activations, positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.min(), values.max(), values.mean(), values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = torch.bincount(indices.flatten(), minlength=HIDDEN_SIZE * 2)\n",
    "\n",
    "sorted_indices = torch.sort(counts, descending=True).indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_feature_activations(text, feature_idx, model, sae, tokenizer):\n",
    "    RESET = \"\\033[0m\"\n",
    "    def get_color(value):\n",
    "        normalized = max(0.0, min(1.0, math.log(value + 1e-5)))\n",
    "        \n",
    "        text_red = int(30 + 225 * normalized)  # increased range (30-255)\n",
    "        \n",
    "        bg_red = 255\n",
    "        bg_green = bg_blue = int(255 - (80 * normalized))  # doubled the reduction (255-175)\n",
    "        \n",
    "        return f\"\\033[48;2;{bg_red};{bg_green};{bg_blue}m\\033[38;2;{text_red};0;0m\"\n",
    "    \n",
    "    # Tokenize and get model outputs\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens, output_hidden_states=True)\n",
    "        activations = outputs.hidden_states[LAYER]\n",
    "        activations = activations.reshape(-1, model.config.hidden_size)\n",
    "        positions = torch.tensor([LAYER] * len(activations))\n",
    "        \n",
    "        indices, values = sae.encode(activations, positions)\n",
    "        \n",
    "        # Create feature mask\n",
    "        feature_mask = (indices == feature_idx).any(dim=1)\n",
    "        feature_values = torch.zeros_like(feature_mask, dtype=torch.float)\n",
    "        for i, (idx, val) in enumerate(zip(indices, values)):\n",
    "            if feature_idx in idx:\n",
    "                # Get top 5 values for this token\n",
    "                top5_mask = torch.zeros_like(val, dtype=torch.bool)\n",
    "                top5_indices = torch.topk(val, min(5, len(val))).indices\n",
    "                top5_mask[top5_indices] = True\n",
    "                \n",
    "                # Check if our feature is in top 5\n",
    "                if top5_mask[idx == feature_idx].any():\n",
    "                    feature_values[i] = val[idx == feature_idx].max()\n",
    "    \n",
    "    # Decode tokens and apply colors\n",
    "    tokens = tokens[0].cpu().numpy()\n",
    "    decoded = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        word = tokenizer.decode([token])\n",
    "        if feature_values[i] > 0:  # Only highlight if it was in top 5\n",
    "            color = get_color(feature_values[i].item())\n",
    "            decoded.append(f\"{color}{word}{RESET}\")\n",
    "        else:\n",
    "            decoded.append(word)\n",
    "    \n",
    "    return \"\".join(decoded)\n",
    "\n",
    "def highlight_feature(text, feature_idx, model, sae, tokenizer):\n",
    "    example_text = tokenizer.decode(text[0], skip_special_tokens=True)\n",
    "    colored_text = highlight_feature_activations(example_text, feature_idx, model, sae, tokenizer)\n",
    "    print(colored_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can at last highlight the features.\n",
    "\n",
    "The most common activations appear to \"overfire\" in this small example we have done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(subset[1], sorted_indices[0], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features I have naively labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(subset[-1], sorted_indices[9], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(subset[2], sorted_indices[89], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(subset[9], sorted_indices[9], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
