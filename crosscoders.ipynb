{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosscoders\n",
    "Attempting to recreate the SAE model Anthropic wrote about recently.\n",
    "\n",
    "Its a SAE which is layer and model agnostic.\n",
    "\n",
    "I'm not sure about the model agnostic part, but I have an idea about layer the layer agnostic part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bjarnihaukurbjarnason/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:333: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(\"Hello World\")\n",
      "print(\"Hello World\")\n",
      "print(\"Hello World\")\n",
      "print(\"Hello World\")\n",
      "print(\"Hello\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"mps\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-0.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-Coder-0.5B\").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "tokens = tokenizer.encode(\"print(\\\"Hello\", return_tensors=\"pt\").to(DEVICE)\n",
    "generation = model.generate(tokens, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(generation[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can very simply get the hidden states by setting output_hidden_states=True flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, torch.Size([1, 3, 896]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(tokens, output_hidden_states=True)\n",
    "\n",
    "# train SAE on these\n",
    "len(out.hidden_states), out.hidden_states[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can register hooks to modify the activations as we please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(\"Hello-o-- andIt it))))))))))))))\n",
      "print(\"Hello World\")\n",
      "print(\"Hello World\")\n",
      "print(\"Hello World\")\n",
      "print(\"Hello World\")\n",
      "print(\"Hello\n"
     ]
    }
   ],
   "source": [
    "layer = model.model.layers[0].mlp.up_proj\n",
    "\n",
    "hook = layer.register_forward_hook(lambda module, input, output: output * 0)\n",
    "\n",
    "generation = model.generate(tokens, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(generation[0], skip_special_tokens=True))\n",
    "\n",
    "hook.remove()\n",
    "\n",
    "generation = model.generate(tokens, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(generation[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = model.config.num_hidden_layers\n",
    "HIDDEN_SIZE = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1469079, 163230)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_to_chat(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": example['prompt']},\n",
    "        {\"role\": \"assistant\", \"content\": example['response']}\n",
    "    ]\n",
    "    \n",
    "    chat_format = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return {\"chat_text\": chat_format}\n",
    "\n",
    "\n",
    "train_ds = load_dataset(\"nampdn-ai/tiny-codes\", split=\"train\").shuffle(seed=42)\n",
    "train_ds = train_ds.map(format_to_chat)\n",
    "\n",
    "val_ds = train_ds.take(len(train_ds) // 10)\n",
    "train_ds = train_ds.skip(len(train_ds) // 10)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([420, 896])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer.encode(train_ds[0][\"chat_text\"], return_tensors=\"pt\").to(DEVICE)\n",
    "attn = torch.tril(torch.ones(1, x.shape[1], x.shape[1])).bool().to(DEVICE)\n",
    "\n",
    "out = model(x, attention_mask=attn, output_hidden_states=True)\n",
    "\n",
    "# we could use every layer's activations\n",
    "\n",
    "out.hidden_states[0].reshape(-1, HIDDEN_SIZE).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse autoencoder itself is quite simple. It's a traditional autoencoder, but with a twist: instead of 'compressing' the input through a bottleneck, it's overcomplete - meaning it has a larger hidden size than the input size.\n",
    "\n",
    "There are different ways of achieving sparsity in these activations. One popular formulation uses a top_k activation function to ensure only the k most important features remain active, while others methods directly optimize for sparsity through various loss terms.\n",
    "\n",
    "The model is trained to reconstruct its input from these sparse activations using reconstruction loss. We can also evaluate its effectiveness by measuring \"recovered\" loss - essentially comparing the next-token prediction accuracy (via negative log-likelihood) both before and after applying the SAE intervention. This helps us understand how well the SAE preserves the important information from the original representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([420, 896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, top_k=5):\n",
    "        super().__init__()\n",
    "        self.input_size, self.hidden_size, self.top_k = input_size, hidden_size, top_k\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.norm = nn.LayerNorm(hidden_size)  # standardizes each feature independently\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        top_k = torch.topk(x, k=self.top_k)\n",
    "        sparse = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "        sparse.scatter_(1, top_k.indices, top_k.values)\n",
    "        return top_k.indices, top_k.values\n",
    "    \n",
    "    def decode(self, indices, values):\n",
    "        sparse = torch.zeros(indices.shape[0], self.hidden_size).to(indices.device)\n",
    "        sparse.scatter_(1, indices, values)\n",
    "        return self.decoder(sparse)\n",
    "\n",
    "    def forward(self, x):\n",
    "        indices, values = self.encode(x)\n",
    "        return self.decode(indices, values)\n",
    "    \n",
    "sae = SAE(HIDDEN_SIZE, HIDDEN_SIZE * 2).to(DEVICE)\n",
    "sae(out.hidden_states[0].reshape(-1, HIDDEN_SIZE)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am borrowing an idea from diffusion models and will encode at which layer in the transformer model, the SAE is being applied to (in diffusion it would be the level of noise being on the image that it needs to denoise).\n",
    "\n",
    "Here I implement a sinusoidal positional encoding for the layers. One could also use learnable embeddings, but I fear that would \"divide\" the expressiveness of the SAE between these embeddings and the SAE itself. Using static Sinusoidal PE sounds like a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, hidden_size, max_len):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_size, 2) * (-math.log(10000.0) / hidden_size))\n",
    "        \n",
    "        pe = torch.zeros(max_len, hidden_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        return x + self.pe[pos]\n",
    "\n",
    "pos_enc = SinusoidalPositionalEncoding(HIDDEN_SIZE, N_LAYERS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to train\n",
    "\n",
    "Firstly, I dislike the habit people have of doing an entire training epoch before validating (especially since datasets can be huge). So I more often than not do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "SEQ_LEN = 128\n",
    "\n",
    "\n",
    "def cycle(dl):\n",
    "    while True: yield from dl\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tokenized = tokenizer(\n",
    "        [item[\"chat_text\"] for item in batch],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=SEQ_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized,\n",
    "        \"attention_mask\": torch.tril(torch.ones(BATCH_SIZE, SEQ_LEN)).bool()\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "NUM_STEPS_PER_TRAIN_EPOCH = len(train_loader)\n",
    "NUM_STEPS_PER_VAL_EPOCH = len(val_loader)\n",
    "\n",
    "# per epoch\n",
    "INTERLEAVE_EVERY = 100\n",
    "N_INTERLEAVES = NUM_STEPS_PER_TRAIN_EPOCH // INTERLEAVE_EVERY\n",
    "N_VALIDATION_STEPS = NUM_STEPS_PER_TRAIN_EPOCH // N_INTERLEAVES\n",
    "\n",
    "\n",
    "train_iter = cycle(train_loader)\n",
    "val_iter = cycle(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, it feels almost wasteful to only use one random layer per item.\n",
    "\n",
    "But perhaps using multiple layers per position would actually hurt training.\n",
    "\n",
    "Adjacent layers process the same token through successive transformations (and has skip connections), which creates highly correlated features. This correlation is perhaps stronger than the relationship between different positions in the sequence. Using multiple layers per position would likely reduce our batch diversity, as we'd be training on variations of the same underlying features rather than truly independent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/367270 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m step_iter:\n\u001b[1;32m     26\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iter)\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43msae_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36msae_step\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msae_step\u001b[39m(batch):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     all_hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(out\u001b[38;5;241m.\u001b[39mhidden_states)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m     layer_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, N_LAYERS, (BATCH_SIZE,))\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:1165\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1165\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:895\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    883\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    884\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    885\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m         position_embeddings,\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:623\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 623\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:519\u001b[0m, in \u001b[0;36mQwen2SdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 519\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}  \u001b[38;5;66;03m# Specific to RoPE models\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:207\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    205\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    206\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(q) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[0;32m--> 207\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/Documents/Projects/minimal-ml/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:180\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    178\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    179\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optim = Adam(sae.parameters(), lr=1e-3)\n",
    "\n",
    "train_loss, val_loss = float(\"inf\"), float(\"inf\")\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "\n",
    "def sae_step(batch):\n",
    "    with torch.no_grad():\n",
    "        out = model(batch[\"input_ids\"].to(DEVICE), attention_mask=batch[\"attention_mask\"].to(DEVICE), output_hidden_states=True)\n",
    "\n",
    "    all_hidden = torch.stack(out.hidden_states).permute(1, 2, 3, 0)\n",
    "    \n",
    "    layer_indices = torch.randint(0, N_LAYERS, (BATCH_SIZE,))\n",
    "    batch_indices = torch.arange(BATCH_SIZE).repeat_interleave(SEQ_LEN)\n",
    "    seq_indices = torch.arange(SEQ_LEN).repeat(BATCH_SIZE)\n",
    "    \n",
    "    inputs = all_hidden[batch_indices, seq_indices, layer_indices].reshape(-1, HIDDEN_SIZE)\n",
    "    inputs = pos_enc(inputs, layer_indices)\n",
    "    reconstructed = sae(inputs)\n",
    "    return F.mse_loss(reconstructed, inputs)\n",
    "\n",
    "step_iter = tqdm(range(EPOCHS * NUM_STEPS_PER_TRAIN_EPOCH), desc=\"Training\")\n",
    "for step in step_iter:\n",
    "    batch = next(train_iter)\n",
    "    \n",
    "    loss = sae_step(batch)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    step_iter.set_postfix({\"Train Loss\": f\"{train_loss:.4f}\", \"Val Loss\": f\"{val_loss:.4f}\"})\n",
    "    \n",
    "    if step % INTERLEAVE_EVERY == 0:\n",
    "        interleaved_losses = []\n",
    "        for _ in range(N_VALIDATION_STEPS):\n",
    "            with torch.no_grad():\n",
    "                batch = next(val_iter)\n",
    "                loss = sae_step(batch)\n",
    "                interleaved_losses.append(loss.item())\n",
    "                \n",
    "                val_loss = loss.item()\n",
    "                \n",
    "                step_iter.set_postfix({\"Train Loss\": f\"{train_loss:.4f}\", \"Val Loss\": f\"{val_loss:.4f}\"})\n",
    "\n",
    "        val_loss = sum(interleaved_losses) / len(interleaved_losses)\n",
    "        val_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sae.state_dict(), \"sae.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.5)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still trending downwards, only trained this for 20-30 minutes on my mac. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to see whether the SAE is picking up on something interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = val_ds.shuffle(seed=42).select(range(100))\n",
    "positives = [x[\"text\"] for x in subset if x[\"label\"] == 1]\n",
    "negatives = [x[\"text\"] for x in subset if x[\"label\"] == 0]\n",
    "\n",
    "positives = [tokenizer.encode(x, return_tensors=\"pt\") for x in positives]\n",
    "negatives = [tokenizer.encode(x, return_tensors=\"pt\") for x in negatives]\n",
    "\n",
    "len(positives), len(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = N_LAYERS // 2  # maybe the middle is interesting\n",
    "\n",
    "with torch.no_grad():\n",
    "    positive_activations = [model(x.to(DEVICE), output_hidden_states=True).hidden_states[LAYER].reshape(-1, HIDDEN_SIZE) for x in tqdm(positives)]\n",
    "    negative_activations = [model(x.to(DEVICE), output_hidden_states=True).hidden_states[LAYER].reshape(-1, HIDDEN_SIZE) for x in tqdm(negatives)]\n",
    "\n",
    "    positive_activations = torch.cat(positive_activations, dim=0)\n",
    "    negative_activations = torch.cat(negative_activations, dim=0)\n",
    "    \n",
    "    positive_indices, positive_values = sae.encode(positive_activations)\n",
    "    negative_indices, negative_values = sae.encode(negative_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts = torch.bincount(positive_indices.flatten(), minlength=HIDDEN_SIZE * 2)\n",
    "negative_counts = torch.bincount(negative_indices.flatten(), minlength=HIDDEN_SIZE * 2)\n",
    "\n",
    "sorted_indices = torch.sort(positive_counts + negative_counts, descending=True).indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_feature_activations(text, feature_idx, model, sae, tokenizer, device=\"mps\"):\n",
    "    RESET = \"\\033[0m\"\n",
    "    def get_color(value):\n",
    "        normalized = max(0.0, min(1.0, math.log(value + 1e-5)))\n",
    "        \n",
    "        text_red = int(30 + 225 * normalized)  # increased range (30-255)\n",
    "        \n",
    "        bg_red = 255\n",
    "        bg_green = bg_blue = int(255 - (80 * normalized))  # doubled the reduction (255-175)\n",
    "        \n",
    "        return f\"\\033[48;2;{bg_red};{bg_green};{bg_blue}m\\033[38;2;{text_red};0;0m\"\n",
    "    \n",
    "    # Tokenize and get model outputs\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[LAYER]\n",
    "        hidden_states = hidden_states.reshape(-1, model.config.hidden_size)\n",
    "        indices, values = sae.encode(hidden_states)\n",
    "        \n",
    "        # Create feature mask\n",
    "        feature_mask = (indices == feature_idx).any(dim=1)\n",
    "        feature_values = torch.zeros_like(feature_mask, dtype=torch.float)\n",
    "        for i, (idx, val) in enumerate(zip(indices, values)):\n",
    "            if feature_idx in idx:\n",
    "                feature_values[i] = val[idx == feature_idx].max()\n",
    "    \n",
    "    # Decode tokens and apply colors\n",
    "    tokens = tokens[0].cpu().numpy()\n",
    "    decoded = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        word = tokenizer.decode([token])\n",
    "        if feature_mask[i]:\n",
    "            color = get_color(feature_values[i].item())\n",
    "            decoded.append(f\"{color}{word}{RESET}\")\n",
    "        else:\n",
    "            decoded.append(word)\n",
    "    \n",
    "    return \"\".join(decoded)\n",
    "\n",
    "def highlight_feature(text, feature_idx, model, sae, tokenizer, device=\"mps\"):\n",
    "    example_text = tokenizer.decode(text[0], skip_special_tokens=True)\n",
    "    colored_text = highlight_feature_activations(example_text, feature_idx, model, sae, tokenizer)\n",
    "    print(colored_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can at last highlight the features.\n",
    "\n",
    "The most common activations appear to \"overfire\" in this small example we have done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[1], sorted_indices[0], model, sae, tokenizer)\n",
    "highlight_feature(negatives[1], sorted_indices[0], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features I have naively labelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Referring to someone or something\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[2], sorted_indices[8], model, sae, tokenizer)\n",
    "highlight_feature(negatives[2], sorted_indices[8], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"End of sentence\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[3], sorted_indices[10], model, sae, tokenizer)\n",
    "highlight_feature(negatives[3], sorted_indices[10], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"the\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[4], sorted_indices[14], model, sae, tokenizer)\n",
    "highlight_feature(negatives[4], sorted_indices[14], model, sae, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"name\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[5], sorted_indices[16], model, sae, tokenizer)\n",
    "highlight_feature(negatives[5], sorted_indices[16], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"media\" feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[6], sorted_indices[20], model, sae, tokenizer)\n",
    "highlight_feature(negatives[6], sorted_indices[20], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"<'br' /><'br' />\" feature, for some reason it was not filtered out of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[7], sorted_indices[22], model, sae, tokenizer)\n",
    "highlight_feature(negatives[7], sorted_indices[22], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"counts of something\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[8], sorted_indices[24], model, sae, tokenizer)\n",
    "highlight_feature(negatives[8], sorted_indices[24], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"description of quality\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[9], sorted_indices[28], model, sae, tokenizer)\n",
    "highlight_feature(negatives[9], sorted_indices[28], model, sae, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a \"a\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[10], sorted_indices[30], model, sae, tokenizer)\n",
    "highlight_feature(negatives[10], sorted_indices[30], model, sae, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"this/these/that/those\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_feature(positives[11], sorted_indices[40], model, sae, tokenizer)\n",
    "highlight_feature(negatives[11], sorted_indices[40], model, sae, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
